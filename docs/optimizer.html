<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>optfx.optimizer API documentation</title>
<meta name="description" content="The optimize function computes the size of the optimization problem by summing
the sizes of the initial conditions, and then creates an instance of â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>optfx.optimizer</code></h1>
</header>
<section id="section-intro">
<p>The optimize function computes the size of the optimization problem by summing
the sizes of the initial conditions, and then creates an instance of the nlopt optimizer using
the specified algorithm. The function then splits the optimization problem into subproblems
based on the size of the initial conditions,
and solves each subproblem separately using the minimize method of the nlopt optimizer.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#! /usr/bin/python3
# -*- coding: utf-8 -*-
&#39;&#39;&#39;The optimize function computes the size of the optimization problem by summing 
the sizes of the initial conditions, and then creates an instance of the nlopt optimizer using 
the specified algorithm. The function then splits the optimization problem into subproblems 
based on the size of the initial conditions, 
and solves each subproblem separately using the minimize method of the nlopt optimizer.
&#39;&#39;&#39;
from dolfin import *
from dolfin_adjoint import *
import numpy as np
from .utils import from_numpy, to_numpy
try:
    import nlopt as nl
except ImportError:
    raise ImportError(&#39;Optimizer depends on Nlopt.&#39;)

class Optimizer(nl.opt):
    &#39;&#39;&#39;
    The Optimizer class is a subclass of the nlopt.opt class.
    It creates an instance of the nlopt optimizer using the specified algorithm.
    &#39;&#39;&#39;
    def __init__(self, problem, initials, wrt, algorithm=&#39;LD_MMA&#39;, *args):
        &#39;&#39;&#39; Creates an instance of the nlopt optimizer using the specified algorithm.

    Available algorithms:
        - LD_MMA (default) Method of moving asymptotes. 
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes
        - LD_SLSQP Sequential quadratic programming.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp-sequential-quadratic-programming
        - LD_LBFGS Limited-memory BFGS.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#lbfgs-limited-memory-bfgs
        - LD_TNEWTON Truncated Newton.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-truncated-newton
        - LD_TNEWTON_RESTART Truncated Newton with restarting.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-restart-truncated-newton-with-restarting
        - LD_TNEWTON_PRECOND Preconditioned truncated Newton.  
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-preconditioned-truncated-newton
        - LD_TNEWTON_PRECOND_RESTART Preconditioned truncated Newton with restarting.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-restart-preconditioned-truncated-newton-with-restarting
        - LD_VAR1 Variably dimensioned solver.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var1-variably-dimensioned-solver
        - LD_VAR2 Variably dimensioned solver.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var2-variably-dimensioned-solver
        - LD_AUGLAG Augmented Lagrangian.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-augmented-lagrangian
        - LD_AUGLAG_EQ Augmented Lagrangian with equality constraints.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-eq-augmented-lagrangian-with-equality-constraints

        Args:
            algorithm: The name of the nlopt algorithm to use.
            problem: An instance of the OptimizationProblem class.
            initials: A list of the initial conditions.
            wrt: A dict of the indices of the control variables for which sensitivities need to be computed.
            *args: Additional arguments to be passed to the nlopt optimizer.

        Returns:
            An instance of the nlopt optimizer.
            
        &#39;&#39;&#39;
        self.initials = initials
        split_index = []
        index = 0
        for initial in self.initials:
            index += initial.vector().size()
            split_index.append(index)
        problemSize = 0
        for initial in self.initials:
            problemSize += initial.vector().size()
        super().__init__(getattr(nl, algorithm), problemSize, *args)
        self.initial_numpy = np.concatenate([to_numpy(i) for i in initials])

        def eval(x, grad):
            xs = np.split(x, split_index)
            xs_fenics = [from_numpy(i, j) for i, j in zip(xs, initials)]
            cost = problem.forward(xs_fenics)
            grad[:] = np.concatenate(problem.backward())
            return cost

        def generate_cost_function(attribute, problem, wrt):
            def cost(x, grad):
                measure = getattr(problem, attribute)()
                grad[:] = np.concatenate(problem.backward_constraint(attribute, wrt[attribute]))
                return measure
            return cost
    
        for attribute in dir(problem):
            if attribute.startswith(&#39;constraint&#39;):
                cost_func = generate_cost_function(attribute, problem, wrt)
                print(type(cost_func))
                self.add_inequality_constraint(cost_func, 1e-8)
        
        self.set_min_objective(eval)
        pass

    def run(self):
        &#39;&#39;&#39; Runs the optimization algorithm and returns the solution as a tuple of fenics functions.

        Returns:
            A tuple of fenics functions.
        
        &#39;&#39;&#39;
        split_index = []
        index = 0
        for initial in self.initials:
            index += initial.vector().size()
            split_index.append(index)
        solution_numpy = self.optimize(self.initial_numpy)
        solution_fenics = [from_numpy(i, j) for i, j in zip(np.split(solution_numpy, split_index), self.initials)]
        return tuple(solution_fenics)




def optimize(problem, initials, wrt, setting, params, algorithm=&#39;LD_MMA&#39;):
    &#39;&#39;&#39;Solves an optimization problem using the specified algorithm.
    
    Available algorithms:
        - LD_MMA (default) Method of moving asymptotes. 
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes
        - LD_SLSQP Sequential quadratic programming.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp-sequential-quadratic-programming
        - LD_LBFGS Limited-memory BFGS.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#lbfgs-limited-memory-bfgs
        - LD_TNEWTON Truncated Newton.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-truncated-newton
        - LD_TNEWTON_RESTART Truncated Newton with restarting.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-restart-truncated-newton-with-restarting
        - LD_TNEWTON_PRECOND Preconditioned truncated Newton.  
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-preconditioned-truncated-newton
        - LD_TNEWTON_PRECOND_RESTART Preconditioned truncated Newton with restarting.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-restart-preconditioned-truncated-newton-with-restarting
        - LD_VAR1 Variably dimensioned solver.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var1-variably-dimensioned-solver
        - LD_VAR2 Variably dimensioned solver.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var2-variably-dimensioned-solver
        - LD_AUGLAG Augmented Lagrangian.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-augmented-lagrangian
        - LD_AUGLAG_EQ Augmented Lagrangian with equality constraints.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-eq-augmented-lagrangian-with-equality-constraints

    Args:
        problem: An instance of the OptimizationProblem class.
        initials: A list of the initial conditions.
        wrt: A list of the indices of the control variables for which sensitivities need to be computed.
        setting: A dictionary of settings for the nlopt optimizer.
        params: A dictionary of parameters for the nlopt optimizer.
        algorithm: The name of the nlopt algorithm to use.

    Returns:
        A list of NumPy arrays containing the optimized control variables.
    &#39;&#39;&#39;
    problem_size = 0

    for initial in initials:
        problem_size += initial.vector().size()

    optimizer = nl.opt(getattr(nl, algorithm), problem_size)

    split_index = []
    index = 0
    for initial in initials:
        index += initial.vector().size()
        split_index.append(index)

    def eval(x, grad):
        xs = np.split(x, split_index)
        xs_fenics = [from_numpy(i, j) for i, j in zip(xs, initials)]
        cost = problem.forward(xs_fenics)
        grad[:] = np.concatenate(problem.backward())
        return cost

    def generate_cost_function(attribute, problem, wrt):
        def cost(x, grad):
            measure = getattr(problem, attribute)()
            grad[:] = np.concatenate(problem.backward_constraint(attribute, wrt[attribute]))
            return measure
        return cost
    
    for attribute in dir(problem):
        if attribute.startswith(&#39;constraint&#39;):
            cost_func = generate_cost_function(attribute, problem, wrt)
            print(type(cost_func))
            optimizer.add_inequality_constraint(cost_func, 1e-8)

    optimizer.set_min_objective(eval)
    for set in setting:
        getattr(optimizer, set)(setting[set])
    for param in params:
        optimizer.set_param(param, params[param])
    initial_numpy = np.concatenate([to_numpy(i) for i in initials])
    solution_numpy = optimizer.optimize(initial_numpy)
    solution_fenics = [from_numpy(i, j) for i, j in zip(np.split(solution_numpy, split_index), initials)]
    return tuple(solution_fenics)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="optfx.optimizer.optimize"><code class="name flex">
<span>def <span class="ident">optimize</span></span>(<span>problem, initials, wrt, setting, params, algorithm='LD_MMA')</span>
</code></dt>
<dd>
<div class="desc"><p>Solves an optimization problem using the specified algorithm.</p>
<p>Available algorithms:
- LD_MMA (default) Method of moving asymptotes.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes</a>
- LD_SLSQP Sequential quadratic programming.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp-sequential-quadratic-programming">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp-sequential-quadratic-programming</a>
- LD_LBFGS Limited-memory BFGS.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#lbfgs-limited-memory-bfgs">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#lbfgs-limited-memory-bfgs</a>
- LD_TNEWTON Truncated Newton.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-truncated-newton">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-truncated-newton</a>
- LD_TNEWTON_RESTART Truncated Newton with restarting.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-restart-truncated-newton-with-restarting">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-restart-truncated-newton-with-restarting</a>
- LD_TNEWTON_PRECOND Preconditioned truncated Newton.<br>
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-preconditioned-truncated-newton">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-preconditioned-truncated-newton</a>
- LD_TNEWTON_PRECOND_RESTART Preconditioned truncated Newton with restarting.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-restart-preconditioned-truncated-newton-with-restarting">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-restart-preconditioned-truncated-newton-with-restarting</a>
- LD_VAR1 Variably dimensioned solver.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var1-variably-dimensioned-solver">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var1-variably-dimensioned-solver</a>
- LD_VAR2 Variably dimensioned solver.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var2-variably-dimensioned-solver">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var2-variably-dimensioned-solver</a>
- LD_AUGLAG Augmented Lagrangian.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-augmented-lagrangian">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-augmented-lagrangian</a>
- LD_AUGLAG_EQ Augmented Lagrangian with equality constraints.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-eq-augmented-lagrangian-with-equality-constraints">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-eq-augmented-lagrangian-with-equality-constraints</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>problem</code></strong></dt>
<dd>An instance of the OptimizationProblem class.</dd>
<dt><strong><code>initials</code></strong></dt>
<dd>A list of the initial conditions.</dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>A list of the indices of the control variables for which sensitivities need to be computed.</dd>
<dt><strong><code>setting</code></strong></dt>
<dd>A dictionary of settings for the nlopt optimizer.</dd>
<dt><strong><code>params</code></strong></dt>
<dd>A dictionary of parameters for the nlopt optimizer.</dd>
<dt><strong><code>algorithm</code></strong></dt>
<dd>The name of the nlopt algorithm to use.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A list of NumPy arrays containing the optimized control variables.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optimize(problem, initials, wrt, setting, params, algorithm=&#39;LD_MMA&#39;):
    &#39;&#39;&#39;Solves an optimization problem using the specified algorithm.
    
    Available algorithms:
        - LD_MMA (default) Method of moving asymptotes. 
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes
        - LD_SLSQP Sequential quadratic programming.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp-sequential-quadratic-programming
        - LD_LBFGS Limited-memory BFGS.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#lbfgs-limited-memory-bfgs
        - LD_TNEWTON Truncated Newton.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-truncated-newton
        - LD_TNEWTON_RESTART Truncated Newton with restarting.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-restart-truncated-newton-with-restarting
        - LD_TNEWTON_PRECOND Preconditioned truncated Newton.  
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-preconditioned-truncated-newton
        - LD_TNEWTON_PRECOND_RESTART Preconditioned truncated Newton with restarting.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-restart-preconditioned-truncated-newton-with-restarting
        - LD_VAR1 Variably dimensioned solver.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var1-variably-dimensioned-solver
        - LD_VAR2 Variably dimensioned solver.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var2-variably-dimensioned-solver
        - LD_AUGLAG Augmented Lagrangian.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-augmented-lagrangian
        - LD_AUGLAG_EQ Augmented Lagrangian with equality constraints.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-eq-augmented-lagrangian-with-equality-constraints

    Args:
        problem: An instance of the OptimizationProblem class.
        initials: A list of the initial conditions.
        wrt: A list of the indices of the control variables for which sensitivities need to be computed.
        setting: A dictionary of settings for the nlopt optimizer.
        params: A dictionary of parameters for the nlopt optimizer.
        algorithm: The name of the nlopt algorithm to use.

    Returns:
        A list of NumPy arrays containing the optimized control variables.
    &#39;&#39;&#39;
    problem_size = 0

    for initial in initials:
        problem_size += initial.vector().size()

    optimizer = nl.opt(getattr(nl, algorithm), problem_size)

    split_index = []
    index = 0
    for initial in initials:
        index += initial.vector().size()
        split_index.append(index)

    def eval(x, grad):
        xs = np.split(x, split_index)
        xs_fenics = [from_numpy(i, j) for i, j in zip(xs, initials)]
        cost = problem.forward(xs_fenics)
        grad[:] = np.concatenate(problem.backward())
        return cost

    def generate_cost_function(attribute, problem, wrt):
        def cost(x, grad):
            measure = getattr(problem, attribute)()
            grad[:] = np.concatenate(problem.backward_constraint(attribute, wrt[attribute]))
            return measure
        return cost
    
    for attribute in dir(problem):
        if attribute.startswith(&#39;constraint&#39;):
            cost_func = generate_cost_function(attribute, problem, wrt)
            print(type(cost_func))
            optimizer.add_inequality_constraint(cost_func, 1e-8)

    optimizer.set_min_objective(eval)
    for set in setting:
        getattr(optimizer, set)(setting[set])
    for param in params:
        optimizer.set_param(param, params[param])
    initial_numpy = np.concatenate([to_numpy(i) for i in initials])
    solution_numpy = optimizer.optimize(initial_numpy)
    solution_fenics = [from_numpy(i, j) for i, j in zip(np.split(solution_numpy, split_index), initials)]
    return tuple(solution_fenics)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="optfx.optimizer.Optimizer"><code class="flex name class">
<span>class <span class="ident">Optimizer</span></span>
<span>(</span><span>problem, initials, wrt, algorithm='LD_MMA', *args)</span>
</code></dt>
<dd>
<div class="desc"><p>The Optimizer class is a subclass of the nlopt.opt class.
It creates an instance of the nlopt optimizer using the specified algorithm.</p>
<p>Creates an instance of the nlopt optimizer using the specified algorithm.</p>
<p>Available algorithms:
- LD_MMA (default) Method of moving asymptotes.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes</a>
- LD_SLSQP Sequential quadratic programming.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp-sequential-quadratic-programming">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp-sequential-quadratic-programming</a>
- LD_LBFGS Limited-memory BFGS.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#lbfgs-limited-memory-bfgs">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#lbfgs-limited-memory-bfgs</a>
- LD_TNEWTON Truncated Newton.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-truncated-newton">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-truncated-newton</a>
- LD_TNEWTON_RESTART Truncated Newton with restarting.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-restart-truncated-newton-with-restarting">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-restart-truncated-newton-with-restarting</a>
- LD_TNEWTON_PRECOND Preconditioned truncated Newton.<br>
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-preconditioned-truncated-newton">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-preconditioned-truncated-newton</a>
- LD_TNEWTON_PRECOND_RESTART Preconditioned truncated Newton with restarting.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-restart-preconditioned-truncated-newton-with-restarting">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-restart-preconditioned-truncated-newton-with-restarting</a>
- LD_VAR1 Variably dimensioned solver.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var1-variably-dimensioned-solver">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var1-variably-dimensioned-solver</a>
- LD_VAR2 Variably dimensioned solver.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var2-variably-dimensioned-solver">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var2-variably-dimensioned-solver</a>
- LD_AUGLAG Augmented Lagrangian.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-augmented-lagrangian">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-augmented-lagrangian</a>
- LD_AUGLAG_EQ Augmented Lagrangian with equality constraints.
see <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-eq-augmented-lagrangian-with-equality-constraints">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-eq-augmented-lagrangian-with-equality-constraints</a></p>
<pre><code>Args:
    algorithm: The name of the nlopt algorithm to use.
    problem: An instance of the OptimizationProblem class.
    initials: A list of the initial conditions.
    wrt: A dict of the indices of the control variables for which sensitivities need to be computed.
    *args: Additional arguments to be passed to the nlopt optimizer.

Returns:
    An instance of the nlopt optimizer.
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Optimizer(nl.opt):
    &#39;&#39;&#39;
    The Optimizer class is a subclass of the nlopt.opt class.
    It creates an instance of the nlopt optimizer using the specified algorithm.
    &#39;&#39;&#39;
    def __init__(self, problem, initials, wrt, algorithm=&#39;LD_MMA&#39;, *args):
        &#39;&#39;&#39; Creates an instance of the nlopt optimizer using the specified algorithm.

    Available algorithms:
        - LD_MMA (default) Method of moving asymptotes. 
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes
        - LD_SLSQP Sequential quadratic programming.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp-sequential-quadratic-programming
        - LD_LBFGS Limited-memory BFGS.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#lbfgs-limited-memory-bfgs
        - LD_TNEWTON Truncated Newton.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-truncated-newton
        - LD_TNEWTON_RESTART Truncated Newton with restarting.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-restart-truncated-newton-with-restarting
        - LD_TNEWTON_PRECOND Preconditioned truncated Newton.  
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-preconditioned-truncated-newton
        - LD_TNEWTON_PRECOND_RESTART Preconditioned truncated Newton with restarting.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#tnewton-precond-restart-preconditioned-truncated-newton-with-restarting
        - LD_VAR1 Variably dimensioned solver.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var1-variably-dimensioned-solver
        - LD_VAR2 Variably dimensioned solver.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#var2-variably-dimensioned-solver
        - LD_AUGLAG Augmented Lagrangian.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-augmented-lagrangian
        - LD_AUGLAG_EQ Augmented Lagrangian with equality constraints.
            see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#auglag-eq-augmented-lagrangian-with-equality-constraints

        Args:
            algorithm: The name of the nlopt algorithm to use.
            problem: An instance of the OptimizationProblem class.
            initials: A list of the initial conditions.
            wrt: A dict of the indices of the control variables for which sensitivities need to be computed.
            *args: Additional arguments to be passed to the nlopt optimizer.

        Returns:
            An instance of the nlopt optimizer.
            
        &#39;&#39;&#39;
        self.initials = initials
        split_index = []
        index = 0
        for initial in self.initials:
            index += initial.vector().size()
            split_index.append(index)
        problemSize = 0
        for initial in self.initials:
            problemSize += initial.vector().size()
        super().__init__(getattr(nl, algorithm), problemSize, *args)
        self.initial_numpy = np.concatenate([to_numpy(i) for i in initials])

        def eval(x, grad):
            xs = np.split(x, split_index)
            xs_fenics = [from_numpy(i, j) for i, j in zip(xs, initials)]
            cost = problem.forward(xs_fenics)
            grad[:] = np.concatenate(problem.backward())
            return cost

        def generate_cost_function(attribute, problem, wrt):
            def cost(x, grad):
                measure = getattr(problem, attribute)()
                grad[:] = np.concatenate(problem.backward_constraint(attribute, wrt[attribute]))
                return measure
            return cost
    
        for attribute in dir(problem):
            if attribute.startswith(&#39;constraint&#39;):
                cost_func = generate_cost_function(attribute, problem, wrt)
                print(type(cost_func))
                self.add_inequality_constraint(cost_func, 1e-8)
        
        self.set_min_objective(eval)
        pass

    def run(self):
        &#39;&#39;&#39; Runs the optimization algorithm and returns the solution as a tuple of fenics functions.

        Returns:
            A tuple of fenics functions.
        
        &#39;&#39;&#39;
        split_index = []
        index = 0
        for initial in self.initials:
            index += initial.vector().size()
            split_index.append(index)
        solution_numpy = self.optimize(self.initial_numpy)
        solution_fenics = [from_numpy(i, j) for i, j in zip(np.split(solution_numpy, split_index), self.initials)]
        return tuple(solution_fenics)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>nlopt.opt</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="optfx.optimizer.Optimizer.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the optimization algorithm and returns the solution as a tuple of fenics functions.</p>
<h2 id="returns">Returns</h2>
<p>A tuple of fenics functions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self):
    &#39;&#39;&#39; Runs the optimization algorithm and returns the solution as a tuple of fenics functions.

    Returns:
        A tuple of fenics functions.
    
    &#39;&#39;&#39;
    split_index = []
    index = 0
    for initial in self.initials:
        index += initial.vector().size()
        split_index.append(index)
    solution_numpy = self.optimize(self.initial_numpy)
    solution_fenics = [from_numpy(i, j) for i, j in zip(np.split(solution_numpy, split_index), self.initials)]
    return tuple(solution_fenics)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="optfx" href="index.html">optfx</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="optfx.optimizer.optimize" href="#optfx.optimizer.optimize">optimize</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="optfx.optimizer.Optimizer" href="#optfx.optimizer.Optimizer">Optimizer</a></code></h4>
<ul class="">
<li><code><a title="optfx.optimizer.Optimizer.run" href="#optfx.optimizer.Optimizer.run">run</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>